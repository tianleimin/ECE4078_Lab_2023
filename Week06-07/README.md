# Milestone 3: Object Recognition and Localisation
- [Introduction](#introduction)
- [Supporting scripts](#supporting-scripts)
- [Data collection (Week 6)](#step-1-data-collection-week-6)
- [Training the detector (Week 6)](#step-2-training-your-yolo-model-week-6)
- [Estimating the target poses (Week 7)](#step-3-estimating-target-poses-week-7)

---
## Introduction
In M3, your PenguinPi robot will learn to recognise different types of fruit and veg via its camera. It will also estimate where these objects are located in the supermarket (arena) based on the actual size of the object, the size of the bounding box of the observed object in its camera view, and its own location at the time of observation using SLAM (M2). 

There are 8 possible types of fruits and vegs for the classification model to recognise from, namely orange, lemon, lime, tomato, yellow capsicum, potato, pumpkin, and garlic. There will be 10 objects in the 3mx3m arena in addition to the 10 ARUCO markers. There may be duplicate objects from the same type, or types of object not appearing in the arena, e.g., 2 oranges, 2 lemon, 1 lime, 1 tomato, 2 capsicum, 1 pumpkin, 1 garlic (no potato).

![target mugshot](Screenshots/TargetMugshot.jpg)

**You will need to install a new Python package to your PenguinPi venv to use YOLO:**
```
PenguinPi\Scripts\activate
python -m pip install ultralytics
```

In Week 6, you will collect and annotate image data to train a fruit&veg classifier using [the YOLOV8 model](https://github.com/ultralytics/ultralytics). Make sure you take enough photos of the fruit and veg models during your Week 6 lab session so that you can work on data annotation and model training outside of the lab. You will also measure the fruit and veg models to get their groundtruth dimensions, which will be used for estimating their location in Week 7.

In Week 7, you will complete codes to estimate the location of detected objects and generate a map of the estimated locations of the 10 fruits and vegs in the arena by driving your robot around the arena, making use of your trained fruit&veg classifier, as well as your M1 and M2 codes. 

Your M3 will be marked based on the accuracy of your fruit&veg classifier on a set of testing photos, as well as the performance of the generated object location map during a live demo. Please see the [M3 marking instructions](M3_marking.md) for detailed info.

![M3 GUI](Screenshots/M3_GUI.png)

Keyboard commands related to running M3 (```python operate.py --ip 192.168.50.1 --port 8080```). Inside the GUI:
- press arrow keys and space for driving the robot (M1)
- press ENTER to run SLAM, ```s``` to save the SLAM map at any time (M2)
- press ```i``` to take a picture (for collecting training data, saved under the ```pibot_dataset``` folder, doesn't require a trained YOLO model)
- press ```p``` to run the trained YOLO object detector (bounding box visualised in bottom left view)
- press ```n``` to save the robot's current pose estimated by SLAM (appending to "lab_output/image.txt") and the corresponding observation image (as "lab_output/pred_*.png")

---
## Supporting scripts
- [operate.py](operate.py) is the central control program that combines keyboard teleoperation (M1), SLAM (M2), and object recognitions (M3). **You need to copy over the [utility scripts](../Week01-02/util) and the [GUI pics](../Week01-02/pics) from M1, and the [calibration parameters](../Week03-05/calibration/param/) and [SLAM scripts](../Week03-05/slam/) from M2 to run it.** In addition, it calls for the object detector that you will develop in M3.
- [YOLO](YOLO/) is an example trained YOLO detector, which can sort of recognise garlic, pumpkin, and orange. You'll need to [train your own YOLO detector](#step-2-training-your-yolo-model-week-6) to recognises all 8 target types with better accuracy.
- [lab_output](lab_output/) contains example outputs when running M3, including [images.txt](lab_output/images.txt) matching each image observation "pred_*.png" with the robot's pose estimated by SLAM when that image was taken, and [targets.txt](lab_output/targets.txt) which is the estimated map of the objects generated by [TargetPoseEst.py](TargetPoseEst.py)
- [image background randomiser](image_background_randomiser/) is an optional tool for you to create training images with randomised backgrounds
- [TargetPoseEst.py](TargetPoseEst.py) is the script that you'll need to modify to generate estimated target location map in [week 7](#step-3-estimating-target-poses-week-7)


---
## Step 1: Data collection (week 6)
In addition to collecting training images, remember to measure the dimensions of the 8 fruit&veg models and update your [target_dimensions_dict](TargetPoseEst.py#L38), which you'll need for estimating the target map in week 7.

### 1.1 Collecting training images with your PenguinPi robot
To take images with the PenguinPi robot, first you need to run the [operate.py](operate.py) script. Then, press  ```i``` 
to take a picture with the PenguinPi robot and the image will be saved to the ```pibot_dataset``` folder. Our main goal 
is to collect images of the fruits and vegs in different orientations, light conditions and backgrounds. The more variety
you have in your dataset, the more robust your model will be.

To simplify the data collection process (instead of manually taking 1000+ images), one approach is to do the following steps:
1. Take a small collections of photos of each fruit and veggie at different orientations with simple backgrounds (e.g. a white sheet of paper)

![pumpkin raw image](Screenshots/pumpkin_raw_image.png)

2. Remove the photo background using [this tool](https://www.remove.bg/) or other software of your choice

![pumpkin no background](Screenshots/pumpkin_no_bg.png)

3. Take a collections of background images of the lab with the robot, or download some random images online
4. Use the [image background randomiser](image_background_randomiser/image_generator.py) script provided to you to generate a large number of images with random backgrounds

### 1.2 Annotating your dataset
To annotate your dataset, we recommend using [Roboflow](https://roboflow.com/) as it is free to use. You can upload your 
images and annotate them using the web interface. You can then export the annotations in YOLO format. However, there are 
many other free annotation tools available online, feel free to use any tool you like.

#### Annotation tips:
- Make sure your bounding boxes are **tight** around the object, as shown in the example below
![roboflow_annotation](Screenshots/roboflow_annotation.png)
- Roboflow has the [assisted labelling](https://blog.roboflow.com/announcing-label-assist/) feature which can speed up the annotation process. You may start with labelling ~30 image per class manually, then train a model and use the assisted labelling feature to label the rest of the images.

### 1.3 Generating dataset on Roboflow
After you have annotated your dataset, you can use Roboflow to generate a dataset in YOLO format. 

![roboflow_generate_dataset](Screenshots/roboflow_generate_dataset.png)

We recommend you to resize the image to 320 x 320 pixels, as the output resolution of the PenguinPi camera is 640 x 480 pixels.

Roboflow also provide option to augment your dataset, which can help improve the robustness of your model.
Have a think about what augmentations are meaningful for our use case, you may add as many augmentations as you like.

---
## Step 2: Training your YOLO model (week 6)

See [YOLOv8_training_notebook.ipynb](YOLOv8_training_notebook.ipynb) for how to train your YOLO with the annotated dataset (you can run this notebook on Google Colab or in your local notebook env).

An [example trained YOLO detector](YOLO/) is provided, which can recognise three types of fruit&veg with room for performance improvements. Replace the [example detector model](YOLO/model/yolov8_model.pt) with your own trained model.

You'll need to demonstrate your trained detector model with a set of given marking images as part of the M3 live demo marking.

![YOLO detector output](Screenshots/sample_yolo_detector.png)

---
## Step 3: Estimating target poses (week 7)

### 3.1 Detecting targets with corresponding robot poses
To estimate pose of targets, you will need to run the SLAM component (press ENTER to run) while running the target detector, so that the robot's pose is known. 

Every time you want to perform object detection, press ```p``` to run the detector, then press ```n``` to save the robot's current pose estimated by SLAM (as "lab_output/image.txt") and the corresponding observation image (as "lab_output/pred_*.png"). 

After you have collected the detector's outputs for all targets, you may press "s" to save SLAM map (performance of your SLAM will influence the target pose estimation). You can exit the GUI by pressing ESC.

### 3.2 Generating estimated target map from observations
Run ```python TargetPoseEst.py```, which will read in the observation saved inside lab_output and generate an estimated object map as "lab_output/targets.txt" during a run through the arena to generate an estimated map of all the observed targets.

**Modify [TargetPoseEst.py](TargetPoseEst.py)** to estimate the locations of the 10 fruits and vegs based on the detector's outputs and the robot's poses. 
- You may improve the [estimate_pose function](TargetPoseEst.py#L15) for computing the target pose using the robot's pose, the detector's output, and the target's true dimensions in each image.
- Replace the [merge_estimation function](TargetPoseEst.py#L69) with your own codes to merge the estimations from multiple observations using filtering or clustering approaches instead of always taking the first estimation of a target type.
- The [TargetPoseEst.py](TargetPoseEst.py) generates an estimation result file as "lab_output/targets.txt", which is then used to be compared against the groundtruth map for computing the target pose estimation errors.
- **Make sure your modified "TargetPoseEst.py" generates a "targets.txt" file that is in the same format as the [given example output](lab_output/targets.txt)**. Generating the estimation map in a wrong format may resulting in it not being compatible with the evaluation scripts and thus getting 0pt for the [M3 target_est_score](M3_marking.md#evaluation).

